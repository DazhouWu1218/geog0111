{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt='UCL' src=\"images/ucl_logo.png\" align='center'>\n",
    "\n",
    "\n",
    "[<img src=\"images/noun_post_2109127.svg\" width=\"50\" align='right'>](016_Python_for.ipynb)\n",
    "[<img src=\"images/noun_pre_2109128.svg\" width=\"50\" align='right'>](018_Python_xxx.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 018 Files and other Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction\n",
    "\n",
    "\n",
    "\n",
    "### Purpose\n",
    "\n",
    "In this session, we will learn about files and similar resources. We will introduce the standard Python library [`pathlib`](https://docs.python.org/3/library/pathlib.html) which is how we deal with file paths, as well as the local package [gurlpath](geog0111/gurlpath) derived from [`urlpath`](https://github.com/chrono-meter/urlpath) that allows a similar object-oriented approach with files and other objects on the web. We will also cover opening and closing files, and some simple read- and write-operations.\n",
    "\n",
    "\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "You will need some understanding of the following:\n",
    "\n",
    "\n",
    "* [001 Using Notebooks](001_Notebook_use.ipynb)\n",
    "* [002 Unix](002_Unix.ipynb)\n",
    "* [003 Getting help](003_Help.ipynb)\n",
    "* [010 Variables, comments and print()](010_Python_Introduction.ipynb)\n",
    "* [011 Data types](011_Python_data_types.ipynb) \n",
    "* [012 String formatting](012_Python_strings.ipynb)\n",
    "* [013_Python_string_methods](013_Python_string_methods.ipynb)\n",
    "\n",
    "\n",
    "\n",
    "### Timing\n",
    "\n",
    "The session should take around 40 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resource location\n",
    "\n",
    "We store information on a computer in files, or file-like resources. We will use the term 'file' below to mean either of these concepts, other than specific issues relating to particular types of file/resource.\n",
    "\n",
    "To get information from files, we need to be able to specify some **address** for the file/resource location, along with some way of interacting with the file. These concepts are captured in the idea of a [URI](https://en.wikipedia.org/wiki/Uniform_Resource_Identifier) (Uniform Resource Indicator). You will most likely have come across the related idea of a [Uniform Resource Locator (URL)](https://en.wikipedia.org/wiki/URL), which is a URL such as [https://www.geog.ucl.ac.uk/people/academic-staff/philip-lewis](https://www.geog.ucl.ac.uk/people/academic-staff/philip-lewis)\n",
    "that gives:\n",
    "\n",
    "* the location of the resource: `people/academic-staff/philip-lewis`\n",
    "* the access and interpretation protocol: [`https`](https://en.wikipedia.org/wiki/HTTPS) (secure [`http`](https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol))\n",
    "* the network domain name: [`www.geog.ucl.ac.uk`](https://www.geog.ucl.ac.uk)\n",
    "\n",
    "When we visit this URL using an appropriate tool such as a browser, the tool can access and interpret the information in the resource: in this case, interpret the [html code](https://www.w3schools.com/html) in the file pointed to by the URL.\n",
    "\n",
    "Similarly, we will be used to the idea of accessing `files` on the computer. These may be in the local file system, or on some network or cloud storage that might be accessible from the local file system. An example of such a file would be some Python code file such as \n",
    "[`geog0111/helloWorld.py`](http://localhost:8888/edit/notebooks/geog0111/helloWorld.py).\n",
    "\n",
    "The two most convenient Python libraries to use for accessing files are [`pathlib`](https://docs.python.org/3/library/pathlib.html) for file paths, the extension package [`urlpath`](https://github.com/chrono-meter/urlpath) for URLs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Path`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Path` is part of the `pathlib` package, so in any Python codes, we first must import this into our workspace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may repeat this `import` in the codes below, for illustration purposes. You need only import it once in any file or session though. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `posix`\n",
    "\n",
    "The main class we will use from `pathlib` is `Path`. This provides an object-oriented way of dealing with files and paths, that is portable to any operating system. Recall from [002 Unix](002_Unix.ipynb) that unix-like operating systems use `posix` filenames, separated by forward slash `/` (or just *slash*). Windows uses a backslash `\\`. But *we* want to write Python codes that are generic and shouldn't need to greatly worry about such issues. Using `Path` greatly helps in this regard, though there will always the occasional time we do need to distinguish `posix` and non-`posix` systems.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common `Path` methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start with a table of [commonly-used methods](https://stackabuse.com/introduction-to-the-python-pathlib-module/#:~:text=The%20Pathlib%20module%20can%20deal,(usually%20the%20current%20directory).) using `Path` and give their [Unix equivalents](002_Unix.ipynb), most of which we have already learned.\n",
    "\n",
    "|command| unix-equivalent | purpose|\n",
    "|---|---|---|\n",
    "|`Path.cwd()`| `pwd` |Return path object representing the current working directory|\n",
    "|`Path.home()`| `~`| Return path object representing the home directory|\n",
    "|`Path.stat()`| `ls -l`* | return info about the path|\n",
    "|`Path.chmod()`| `chmod` | change file mode and permissions|\n",
    "|`Path.glob(pattern)`| `ls *` | Glob the pattern given in the directory that is represented by the path, yielding matching files of any kind|\n",
    "|`Path.mkdir()`| `mkdir` | to create a new directory at the given path|\n",
    "|`Path.rename()`| `mv` | Rename a file or directory to the given target|\n",
    "|`Path.rmdir()`| `rmdir` | Remove the empty directory|\n",
    "|`Path.unlink()`| `rm` | Remove the file or symbolic link|\n",
    "\n",
    "`*` Really, `Path.stat()` equates to the `unix` command `stat`, but this contains the information we access using `ls -l`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are already familiar with most of these commands in `unix`, we can get straight into using them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "print(f'I am in directory {Path.cwd()}')\n",
    "print(f'My home is {Path.home()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep the filenames generic, we form a filename using `Path()`, so `Path('bin','README')` would refer to the filename `bin/README` on a `posix` system, and `bin/README` on Windows.\n",
    "\n",
    "### File information\n",
    "\n",
    "The file permissions format we are used to from `ls -l` is accessed through `filename.stat().st_mode` but needs to be converted to octal to match to `ls`\n",
    "\n",
    "    oct(filename.stat().st_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similar information to ls -l\n",
    "readme=Path('bin','README')\n",
    "print(f'README file is {readme}')\n",
    "print(f'       size is {readme.stat().st_size} bytes')\n",
    "print(f'       mode is {oct(readme.stat().st_mode)}')\n",
    "print(f'      owner is {readme.owner()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `glob` generators \n",
    "\n",
    "To use a wildcard (or any pattern) to refer to a list of files, we use `Path.glob()` (or `Path.rglob()` for a recursive list). This function returns a generator, which is a special type of list whereby the items in the list are produced on demand (as we use them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use glob to get a list of filenames in the directory bin \n",
    "# that end with .sh -> pattern *.sh using a wildcard\n",
    "filenames = Path('bin').glob('*.sh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generator does not have all of the attributes of a list, so we cannot, for example, know the length, withjout first converting it tot a list. It is intended that you step through it one item at a time. This is usually done in a `for` loop, or similar construct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,f in enumerate(filenames):\n",
    "    print(f'file {i} is {f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantage of a generator is that it will generally need less memory than fully calculating all items in a list. Once we move on to the next item in the generator, any memory used by current item is freed.\n",
    "\n",
    "But is the size of objects is small, you will not notice much impact if you convert the generator to a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PosixPath('bin/notebook-mkdocs.sh'), PosixPath('bin/setup.sh'), PosixPath('bin/notebook-run.sh'), PosixPath('bin/link-set.sh'), PosixPath('bin/git-remove-all.sh')]\n"
     ]
    }
   ],
   "source": [
    "filenames = list(Path('bin').glob('*.sh'))\n",
    "print(filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use `glob` now to get the file permissions of each file `n*` in the directory `bin`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin/notebook-mkdocs.sh    : 0o100755\n",
      "bin/notebook-run.sh       : 0o100755\n"
     ]
    }
   ],
   "source": [
    "# use glob to get a list of filenames in the directory bin \n",
    "# that end with .sh -> pattern n* using a wildcard\n",
    "filenames = Path('bin').glob('n*')\n",
    "\n",
    "# loop over the filenames and print the permissions\n",
    "# as octal. Note how we use :25s to line items up\n",
    "for f in filenames:\n",
    "    print(f'{str(f):25s} : {oct(f.stat().st_mode)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "#### Exercise 1\n",
    "\n",
    "* Use `Path` to show the file permissions of all files that end `.sh` in the directory `bin`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "# Use Path to show the file permissions of\n",
    "# all files that end .sh in the directory bin\n",
    "\n",
    "# use glob to get a list of filenames in the directory bin \n",
    "# that end with .sh -> pattern *.sh using a wildcard\n",
    "filenames = Path('bin').glob('*.sh')\n",
    "# loop over the filenames\n",
    "for f in filenames:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `absolute` `parts` `name` `parent`\n",
    "\n",
    "We can use `Path` to convert filenames between relative and absolute representations using `absolute()` and `relative_to()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'I am in {Path.cwd()}')\n",
    "\n",
    "# define a relative path name\n",
    "readme=Path('bin','README')\n",
    "print(f'original relative name:\\n\\t{readme}')\n",
    "\n",
    "# convert to absolute\n",
    "readme = readme.absolute()\n",
    "print(f'absolute name:\\n\\t{readme}')\n",
    "\n",
    "# now make a relative pathname, \n",
    "# reletive to current working directory\n",
    "readme = readme.relative_to(Path.cwd())\n",
    "print(f'name relative to {Path.cwd()}:\\n\\t{readme}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite often we need to split some filename up into its constituent parts. This is achieved with `paths` which gives a list of the file name tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readme=Path('bin','README')\n",
    "print(readme.parts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use that to get at the filename `README` from the last item in the list, but a more object-oriented way is to use `name` (and `parent` to get the directory up to that point):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readme=Path('bin','README')\n",
    "print(f'name   of {readme} is {readme.name}')\n",
    "print(f'parent of {readme} is {readme.parent}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "#### Exercise 2\n",
    "\n",
    "* print out the absolute pathname of the directory that `images/ucl.png` is in\n",
    "* print the size of the file in KB to two decimal places\n",
    "\n",
    "You will need to know how many Bytes in a Kilobyte, and how to [format a string to two decimal places](012_Python_strings.ipynb#String-formating)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and writing\n",
    "\n",
    "We can conveniently use `pathlib` to deal with file input and output. The main methods to be aware of are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "|command|  purpose|\n",
    "|---|---|\n",
    "|`Path.open()`| open a file and return a file descriptor|\n",
    "|`Path.read_text()`|  read text|\n",
    "|`Path.write_text()`| write text|\n",
    "|`Path.read_bytes()`| read byte data|\n",
    "|`Path.write_bytes()`| write byte data|\n",
    "\n",
    "\n",
    "### `with ... as ...` `Path.open` `yaml` `json`\n",
    "\n",
    "The first of these provides a file descriptor for the open file. This is used to interface to other input/output functions in Python. A typical example of this is reading a configuration file in [`yaml` format](http://zetcode.com/python/yaml/).\n",
    "\n",
    "The usual way of opening a file to get the file descriptor is:\n",
    "\n",
    "    with Path(filename).open('r') as f:\n",
    "       # do some reading with f\n",
    "       pass\n",
    "       \n",
    "\n",
    "We use the form `with ... as ...` here, so that the file descriptor `f` only exists within this construct and the file is automatically closed when we finish. Codes are spaced in inside the construct, as we have seen in `if ...` or `for ... in ...` constructs.\n",
    "\n",
    "Here, we have set the flag `r` within the `open()` statement (this is the default mode). This means that the file will be opened for *reading* only. Alternatives include `w` for writing, or `w+` for appending.\n",
    "\n",
    "In the following example, we use `Path` to open the file [`bin/copy/environment.yml`](bin/copy/environment.yml) and read it using the `yaml` library. This file specifies which packages are loaded in our Python environment. It has a simple ascii format, but since it is a `yaml` file, we should read it with code that interprets the format correctly and safely into a dictionary. This is done using `yaml.safe_load(f)` with `f` an open file descriptor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# form the file name\n",
    "yaml_file = Path('bin','copy','environment.yml')\n",
    "\n",
    "with yaml_file.open('r') as f:\n",
    "    env = yaml.safe_load(f)\n",
    "\n",
    "print(f'env is type {type(env)}')\n",
    "print(f'env keys: {env.keys()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common file format for configuration information is [`json`](https://www.json.org/json-en.html). We can use the same form of code as above to write the information in `env` into a `json` format file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# form the file name\n",
    "json_file = Path('bin','copy','environment.json')\n",
    "\n",
    "with json_file.open('w') as f:\n",
    "    json.dump(env, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This now exists as [`bin/copy/environment.json`](bin/copy/environment.json)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read and write text\n",
    "\n",
    "To use `Path.write_text()` to write text to a file `work/easy.txt`, we simply do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geog0111.gurlpath import URL\n",
    "from pathlib import Path\n",
    "\n",
    "u = 'https://www.json.org/json-en.html'\n",
    "url = URL(u)\n",
    "data = url.read_text()\n",
    "ofile = Path('data',url.name)\n",
    "print(f'writing to {ofile}')\n",
    "osize = ofile.write_text(data)\n",
    "assert osize == 26718\n",
    "print('passed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `Path.write_text()` to write text to a file `work/easy.txt`, we simply do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = URL('https://www.json.org').get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://www.json.org\n",
    "some_text = '''\n",
    "It is easy for humans to read and write.\n",
    "It is easy for machines to parse and generate. \n",
    "'''\n",
    "\n",
    "# set up the filename\n",
    "outfile = Path('work','easy.txt')\n",
    "# write the text\n",
    "nbytes = outfile.write_text(some_text)\n",
    "# print what we did\n",
    "print(f'wrote {nbytes} bytes to {outfile}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "#### Exercise 3\n",
    "\n",
    "* Using `Path.read_text()` read the text from the file `work/easy.txt` and print the text returned.\n",
    "* split the text into lines of text using `str.split()` at each newline, and print out the resulting list\n",
    "\n",
    "You learned how to split strings in [013_Python_string_methods](013_Python_string_methods.ipynb#split()-and-join())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "# Using Path.read_text() read the text from the file work/easy.txt \n",
    "# and print the text returned.\n",
    "\n",
    "# set up the filename\n",
    "infile = Path('work','easy.txt')\n",
    "# read the text\n",
    "read_text = infile.read_text()\n",
    "\n",
    "# split the text into lines of \n",
    "# text using str.split() at each newline, \n",
    "# and print out the resulting list\n",
    "lines = read_text.split('\\n')\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A similar approach is taken for reading and writing binary data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading from a URL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `gurlpath`\n",
    "\n",
    "The library [`geog0111`](geog0111/geog0111.py) in [geog0111](geog0111) is designed to operate in a similar manner to `pathlib` for reading data from URLs. \n",
    "\n",
    "\n",
    "The object corresponding to `Path` is `URL`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geog0111.gurlpath import URL\n",
    "url = \"https://www.metoffice.gov.uk/hadobs/hadukp/data/monthly/HadSEEP_monthly_qc.txt\"\n",
    "\n",
    "f = URL(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have similar functionality for manipulating filenames, but more limited file information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'URL {f}')\n",
    "print(f'name   : {f.name}')\n",
    "print(f'parent : {f.parent}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading text from a URL\n",
    "\n",
    "It is particularly useful for a simple object-oriented approach to reading text or data from a URL:\n",
    "\n",
    "\n",
    "|command|  purpose|\n",
    "|---|---|\n",
    "|`URL.read_text()`|  read text|\n",
    "|`URL.read_bytes()`| read byte data|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we examine the data on the website [HadSEEP_monthly_qc.txt](https://www.metoffice.gov.uk/hadobs/hadukp/data/monthly/HadSEEP_monthly_qc.txt), we see that the first 3 lines are metedata. The fourth line specifies the data columns, then the rest are datra values, with `-99.9` as invalid.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "url = \"https://www.metoffice.gov.uk/hadobs/hadukp/data/monthly/HadSEEP_monthly_qc.txt\"\n",
    "f = URL(url)\n",
    "\n",
    "text_data = f.read_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could do some processing and manipulation of the text data string. For example the following code will split the string on newline `\\n` characters into a list, take the first 6 lines of the list, then join it back again into a string:\n",
    "\n",
    "    '\\n'.join(text_data.split('\\n')[:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data read is 12915 bytes of text data\n",
      "Monthly Southeast England precipitation (mm). Daily automated values used after 1996.\n",
      "Wigley & Jones (J.Climatol.,1987), Gregory et al. (Int.J.Clim.,1991)\n",
      "Jones & Conway (Int.J.Climatol.,1997), Alexander & Jones (ASL,2001). Values may change after QC.\n",
      "YEAR   JAN   FEB   MAR   APR   MAY   JUN   JUL   AUG   SEP   OCT   NOV   DEC   ANN\n",
      " 1873  87.1  50.4  52.9  19.9  41.1  63.6  53.2  56.4  62.0  86.0  59.4  15.7  647.7\n",
      " 1874  46.8  44.9  15.8  48.4  24.1  49.9  28.3  43.6  79.4  96.1  63.9  52.3  593.5\n"
     ]
    }
   ],
   "source": [
    "print(f'data read is {len(text_data)} bytes of text data')\n",
    "print('\\n'.join(text_data.split('\\n')[:6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is effective, but normally we would use specialised packages designed for reading tabular data of this sort. The most common of these is [pandas](https://pandas.pydata.org/) for data analysis and manipulation. If the online file is a simple format such as [csv]()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 403: Forbidden",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-357eeb25d534>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://www.metoffice.gov.uk/hadobs/hadukp/data/monthly/HadSEEP_monthly_qc.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mskiprows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mna_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m99.9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mr\"[ ]{1,}\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'python'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/geog0111/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/geog0111/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0;31m# See https://github.com/python/mypy/issues/1297\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     fp_or_buf, _, compression, should_close = get_filepath_or_buffer(\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m     )\n\u001b[1;32m    437\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/geog0111/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;31m# TODO: fsspec can also handle HTTP via requests, but leaving this unchanged\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m         \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m         \u001b[0mcontent_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Content-Encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcontent_encoding\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"gzip\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/geog0111/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/geog0111/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/geog0111/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/geog0111/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             response = self.parent.error(\n\u001b[0;32m--> 641\u001b[0;31m                 'http', request, response, code, msg, hdrs)\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/geog0111/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[0;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/geog0111/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/geog0111/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 403: Forbidden"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "url = \"https://www.metoffice.gov.uk/hadobs/hadukp/data/monthly/HadSEEP_monthly_qc.txt\"\n",
    "\n",
    "c=pd.read_csv(url,skiprows=3,na_values=[-99.9],sep=r\"[ ]{1,}\",engine='python')\n",
    "c.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "c=pd.read_table(io.StringIO(f.read_text()),skiprows=3,na_values=[-99.9],sep=r\"[ ]{1,}\",engine='python')\n",
    "c.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(c['YEAR'],c['JAN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# ANSWER \n",
    "# following from above\n",
    "\n",
    "# set up the filename\n",
    "infile = Path('work','easy.txt')\n",
    "# read the text\n",
    "read_text = infile.read_text()\n",
    "\n",
    "# print what we did\n",
    "print(f'read\\n\"\"\"{read_text}\"\"\"\\nfrom {infile}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "import json\n",
    "\n",
    "# show the size of the files \n",
    "# bin/copy/environment.json and bin/copy/environment.yml\n",
    "\n",
    "# form the file names\n",
    "json_file = Path('bin','copy','environment.json')\n",
    "yaml_file = Path('bin','copy','environment.yml')\n",
    "# loop and print size\n",
    "for f in [json_file,yaml_file]:\n",
    "    print(f'{f} : {f.stat().st_size} bytes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "try again ... or enter 'exit' as username to quit\n",
      "--> user login required for https://e4ftl01.cr.usgs.gov/ <--\n",
      "Enter your username: plewis@geog.ucl.ac.uk\n",
      "please type your password········\n",
      "please re-type your password for confirmation········\n",
      "password created\n",
      "--> logging in to https://e4ftl01.cr.usgs.gov/\n",
      "--> data read from https://e4ftl01.cr.usgs.gov/\n",
      "writing to data/MCD15A3H.A2003345.h09v06.006.2015084002115.hdf\n",
      "passed\n"
     ]
    }
   ],
   "source": [
    "from geog0111.gurlpath import URL\n",
    "from pathlib import Path\n",
    "\n",
    "u='https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.11/MCD15A3H.A2003345.h09v06.006.2015084002115.hdf'\n",
    "url = URL(u)\n",
    "#cy = Cylog(url.anchor,verbose=True).login()\n",
    "#print(cy)\n",
    "data = url.read_bytes(verbose=True)\n",
    "ofile = Path('data',url.name)\n",
    "print(f'writing to {ofile}')\n",
    "osize = ofile.write_bytes(data)\n",
    "assert osize == 3365255\n",
    "print('passed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from geog0111.gurlpath import URL\n",
    "from geog0111.cylog import Cylog\n",
    "from pathlib import Path\n",
    "\n",
    "u='https://e4ftl01.cr.usgs.gov'\n",
    "url = URL(u)\n",
    "rlist = url.glob('MOT*/MCD15A3H.006/2003.12.*/*0.hdf',verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to download this set of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wildcards in: ['MOT*' 'MCD15A3H.006' '2003.12.*' '*0.hdf']\n",
      "----> level 0/4 : MOT*\n",
      "--> discovered 1 files with pattern MOT* in https://e4ftl01.cr.usgs.gov\n",
      "----> level 1/4 : MCD15A3H.006\n",
      "--> discovered 1 files with pattern MCD15A3H.006 in https://e4ftl01.cr.usgs.gov/MOTA\n",
      "----> level 2/4 : 2003.12.*\n",
      "--> discovered 7 files with pattern 2003.12.* in https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006\n",
      "----> level 3/4 : *0.hdf\n",
      "--> discovered 32 files with pattern *0.hdf in https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.03\n",
      "--> discovered 30 files with pattern *0.hdf in https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.07\n",
      "--> discovered 22 files with pattern *0.hdf in https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.11\n",
      "--> discovered 22 files with pattern *0.hdf in https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.15\n",
      "--> discovered 18 files with pattern *0.hdf in https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.23\n",
      "--> discovered 24 files with pattern *0.hdf in https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.27\n",
      "--> discovered 33 files with pattern *0.hdf in https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.31\n"
     ]
    }
   ],
   "source": [
    "hdffile = next(rlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.03/MCD15A3H.A2003337.h04v10.006.2015083165740.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.03/MCD15A3H.A2003337.h05v13.006.2015083165740.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.03/MCD15A3H.A2003337.h08v04.006.2015083171130.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.03/MCD15A3H.A2003337.h09v09.006.2015083165730.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.03/MCD15A3H.A2003337.h12v02.006.2015083171110.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.03/MCD15A3H.A2003337.h12v05.006.2015083165720.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.03/MCD15A3H.A2003337.h13v02.006.2015083165540.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.03/MCD15A3H.A2003337.h13v03.006.2015083165800.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.03/MCD15A3H.A2003337.h14v02.006.2015083165730.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.03/MCD15A3H.A2003337.h14v04.006.2015083165740.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.03/MCD15A3H.A2003337.h15v03.006.2015083171120.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.03/MCD15A3H.A2003337.h15v11.006.2015083165730.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.03/MCD15A3H.A2003337.h17v08.006.2015083165810.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.03/MCD15A3H.A2003337.h20v04.006.2015083165750.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.03/MCD15A3H.A2003337.h20v06.006.2015083165730.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.03/MCD15A3H.A2003337.h20v11.006.2015083165740.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.03/MCD15A3H.A2003337.h21v05.006.2015083165700.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.03/MCD15A3H.A2003337.h22v02.006.2015083165750.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.03/MCD15A3H.A2003337.h22v08.006.2015083165800.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.03/MCD15A3H.A2003337.h22v13.006.2015083165740.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.03/MCD15A3H.A2003337.h22v14.006.2015083165740.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.03/MCD15A3H.A2003337.h27v05.006.2015083165720.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.03/MCD15A3H.A2003337.h27v09.006.2015083165750.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.03/MCD15A3H.A2003337.h27v11.006.2015083165720.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.03/MCD15A3H.A2003337.h27v12.006.2015083165850.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.03/MCD15A3H.A2003337.h28v11.006.2015083165730.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.03/MCD15A3H.A2003337.h29v06.006.2015083165600.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.03/MCD15A3H.A2003337.h29v07.006.2015083165650.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.03/MCD15A3H.A2003337.h29v09.006.2015083165710.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.03/MCD15A3H.A2003337.h29v12.006.2015083165750.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.03/MCD15A3H.A2003337.h31v06.006.2015083165720.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.03/MCD15A3H.A2003337.h31v13.006.2015083165700.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.07/MCD15A3H.A2003341.h01v07.006.2015083051520.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.07/MCD15A3H.A2003341.h02v10.006.2015083054140.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.07/MCD15A3H.A2003341.h05v11.006.2015083054140.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.07/MCD15A3H.A2003341.h06v03.006.2015083050240.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.07/MCD15A3H.A2003341.h09v02.006.2015083051540.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.07/MCD15A3H.A2003341.h10v05.006.2015083051750.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.07/MCD15A3H.A2003341.h11v07.006.2015083051730.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.07/MCD15A3H.A2003341.h12v02.006.2015083051710.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.07/MCD15A3H.A2003341.h13v14.006.2015083053110.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.07/MCD15A3H.A2003341.h14v09.006.2015083051100.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.07/MCD15A3H.A2003341.h15v11.006.2015083051530.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.07/MCD15A3H.A2003341.h16v02.006.2015083052050.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.07/MCD15A3H.A2003341.h17v08.006.2015083052040.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.07/MCD15A3H.A2003341.h18v08.006.2015083052500.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.07/MCD15A3H.A2003341.h19v12.006.2015083053110.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.07/MCD15A3H.A2003341.h20v09.006.2015083053150.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.07/MCD15A3H.A2003341.h21v02.006.2015083053830.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.07/MCD15A3H.A2003341.h21v13.006.2015083052830.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.07/MCD15A3H.A2003341.h22v14.006.2015083053110.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.07/MCD15A3H.A2003341.h23v05.006.2015083051020.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.07/MCD15A3H.A2003341.h23v10.006.2015083051220.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.07/MCD15A3H.A2003341.h25v04.006.2015083051640.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.07/MCD15A3H.A2003341.h25v06.006.2015083052150.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.07/MCD15A3H.A2003341.h26v05.006.2015083051710.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.07/MCD15A3H.A2003341.h27v14.006.2015083052730.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.07/MCD15A3H.A2003341.h28v04.006.2015083052740.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.07/MCD15A3H.A2003341.h30v12.006.2015083051520.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.07/MCD15A3H.A2003341.h33v09.006.2015083052740.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.07/MCD15A3H.A2003341.h33v10.006.2015083052440.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.07/MCD15A3H.A2003341.h34v10.006.2015083054140.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.11/MCD15A3H.A2003345.h01v07.006.2015084000250.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.11/MCD15A3H.A2003345.h06v03.006.2015083235840.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.11/MCD15A3H.A2003345.h12v09.006.2015084001540.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.11/MCD15A3H.A2003345.h12v12.006.2015084001500.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.11/MCD15A3H.A2003345.h12v13.006.2015084002120.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.11/MCD15A3H.A2003345.h13v04.006.2015084001500.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.11/MCD15A3H.A2003345.h14v04.006.2015084000300.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.11/MCD15A3H.A2003345.h14v11.006.2015083235900.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.11/MCD15A3H.A2003345.h17v06.006.2015084002050.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.11/MCD15A3H.A2003345.h19v02.006.2015084000250.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.11/MCD15A3H.A2003345.h20v02.006.2015084000240.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.11/MCD15A3H.A2003345.h21v06.006.2015084002100.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.11/MCD15A3H.A2003345.h21v10.006.2015084001750.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.11/MCD15A3H.A2003345.h22v09.006.2015084002120.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.11/MCD15A3H.A2003345.h23v02.006.2015084001530.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.11/MCD15A3H.A2003345.h28v09.006.2015084002100.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.11/MCD15A3H.A2003345.h28v12.006.2015084002020.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.11/MCD15A3H.A2003345.h29v10.006.2015084002130.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.11/MCD15A3H.A2003345.h29v12.006.2015084001520.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.11/MCD15A3H.A2003345.h30v07.006.2015084003750.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.11/MCD15A3H.A2003345.h30v09.006.2015084002850.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.11/MCD15A3H.A2003345.h35v10.006.2015083235850.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.15/MCD15A3H.A2003349.h00v08.006.2015084011510.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.15/MCD15A3H.A2003349.h02v08.006.2015084012930.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.15/MCD15A3H.A2003349.h08v05.006.2015084012300.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.15/MCD15A3H.A2003349.h09v08.006.2015084011510.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.15/MCD15A3H.A2003349.h12v08.006.2015084012250.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.15/MCD15A3H.A2003349.h13v08.006.2015084012240.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.15/MCD15A3H.A2003349.h13v10.006.2015084012300.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.15/MCD15A3H.A2003349.h14v03.006.2015084012240.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.15/MCD15A3H.A2003349.h15v14.006.2015084012230.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.15/MCD15A3H.A2003349.h18v09.006.2015084011520.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.15/MCD15A3H.A2003349.h19v06.006.2015084011520.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.15/MCD15A3H.A2003349.h19v08.006.2015084012310.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.15/MCD15A3H.A2003349.h19v10.006.2015084012300.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.15/MCD15A3H.A2003349.h22v11.006.2015084011520.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.15/MCD15A3H.A2003349.h24v02.006.2015084012240.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.15/MCD15A3H.A2003349.h30v05.006.2015084012230.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.15/MCD15A3H.A2003349.h31v06.006.2015084012230.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.15/MCD15A3H.A2003349.h31v08.006.2015084011520.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.15/MCD15A3H.A2003349.h31v12.006.2015084012230.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.15/MCD15A3H.A2003349.h33v10.006.2015084012230.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.15/MCD15A3H.A2003349.h34v10.006.2015084011520.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.15/MCD15A3H.A2003349.h35v10.006.2015084011510.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.23/MCD15A3H.A2003357.h04v09.006.2015084025450.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.23/MCD15A3H.A2003357.h10v04.006.2015084030030.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.23/MCD15A3H.A2003357.h10v05.006.2015084030040.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.23/MCD15A3H.A2003357.h11v04.006.2015084030030.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.23/MCD15A3H.A2003357.h12v04.006.2015084025450.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.23/MCD15A3H.A2003357.h12v08.006.2015084025450.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.23/MCD15A3H.A2003357.h12v11.006.2015084025500.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.23/MCD15A3H.A2003357.h13v13.006.2015084030020.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.23/MCD15A3H.A2003357.h14v09.006.2015084030020.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.23/MCD15A3H.A2003357.h19v02.006.2015084025440.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.23/MCD15A3H.A2003357.h21v09.006.2015084024900.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.23/MCD15A3H.A2003357.h22v08.006.2015084025040.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.23/MCD15A3H.A2003357.h23v06.006.2015084025440.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.23/MCD15A3H.A2003357.h25v08.006.2015084025440.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.23/MCD15A3H.A2003357.h26v02.006.2015084025440.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.23/MCD15A3H.A2003357.h31v11.006.2015084025500.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.23/MCD15A3H.A2003357.h32v08.006.2015084024850.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.23/MCD15A3H.A2003357.h35v10.006.2015084024840.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.27/MCD15A3H.A2003361.h08v04.006.2015084002830.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.27/MCD15A3H.A2003361.h08v11.006.2015084001230.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.27/MCD15A3H.A2003361.h09v09.006.2015084004520.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.27/MCD15A3H.A2003361.h11v07.006.2015084002130.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.27/MCD15A3H.A2003361.h12v02.006.2015084003740.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.27/MCD15A3H.A2003361.h12v09.006.2015084003750.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.27/MCD15A3H.A2003361.h12v11.006.2015084002150.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.27/MCD15A3H.A2003361.h13v03.006.2015084002850.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.27/MCD15A3H.A2003361.h13v08.006.2015084000320.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.27/MCD15A3H.A2003361.h13v10.006.2015084002900.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.27/MCD15A3H.A2003361.h16v06.006.2015084000230.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.27/MCD15A3H.A2003361.h17v02.006.2015084003730.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.27/MCD15A3H.A2003361.h18v02.006.2015084002130.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.27/MCD15A3H.A2003361.h18v14.006.2015084004520.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.27/MCD15A3H.A2003361.h20v05.006.2015084002130.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.27/MCD15A3H.A2003361.h24v04.006.2015084003750.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.27/MCD15A3H.A2003361.h27v10.006.2015084001510.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.27/MCD15A3H.A2003361.h29v06.006.2015084002830.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.27/MCD15A3H.A2003361.h29v07.006.2015084002130.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.27/MCD15A3H.A2003361.h29v08.006.2015084001000.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.27/MCD15A3H.A2003361.h29v11.006.2015084002200.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.27/MCD15A3H.A2003361.h30v06.006.2015084000950.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.27/MCD15A3H.A2003361.h30v12.006.2015084002840.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.27/MCD15A3H.A2003361.h32v12.006.2015084003740.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.31/MCD15A3H.A2003365.h01v09.006.2015084192700.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.31/MCD15A3H.A2003365.h01v10.006.2015084200740.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.31/MCD15A3H.A2003365.h02v06.006.2015084194840.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.31/MCD15A3H.A2003365.h03v07.006.2015084194850.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.31/MCD15A3H.A2003365.h04v11.006.2015084194850.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.31/MCD15A3H.A2003365.h10v03.006.2015084202850.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.31/MCD15A3H.A2003365.h10v11.006.2015084194840.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.31/MCD15A3H.A2003365.h11v11.006.2015084192720.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.31/MCD15A3H.A2003365.h11v12.006.2015084200730.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.31/MCD15A3H.A2003365.h12v11.006.2015084200820.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.31/MCD15A3H.A2003365.h13v08.006.2015084194840.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.31/MCD15A3H.A2003365.h14v10.006.2015084192720.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.31/MCD15A3H.A2003365.h16v07.006.2015084192800.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.31/MCD15A3H.A2003365.h16v09.006.2015084192710.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.31/MCD15A3H.A2003365.h17v08.006.2015084194910.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.31/MCD15A3H.A2003365.h17v12.006.2015084194850.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.31/MCD15A3H.A2003365.h20v03.006.2015084200830.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.31/MCD15A3H.A2003365.h20v04.006.2015084200740.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.31/MCD15A3H.A2003365.h20v08.006.2015084202830.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.31/MCD15A3H.A2003365.h21v03.006.2015084200810.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.31/MCD15A3H.A2003365.h21v13.006.2015084194850.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.31/MCD15A3H.A2003365.h22v08.006.2015084194910.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.31/MCD15A3H.A2003365.h23v08.006.2015084194850.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.31/MCD15A3H.A2003365.h24v03.006.2015084194920.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.31/MCD15A3H.A2003365.h25v02.006.2015084194900.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.31/MCD15A3H.A2003365.h25v06.006.2015084194930.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.31/MCD15A3H.A2003365.h26v03.006.2015084194850.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.31/MCD15A3H.A2003365.h26v08.006.2015084194900.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.31/MCD15A3H.A2003365.h27v11.006.2015084194900.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.31/MCD15A3H.A2003365.h28v09.006.2015084194910.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.31/MCD15A3H.A2003365.h30v05.006.2015084194900.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.31/MCD15A3H.A2003365.h30v07.006.2015084194900.hdf'),\n",
       " URL('https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2003.12.31/MCD15A3H.A2003365.h35v09.006.2015084194900.hdf')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdffile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "\n",
    "# read the information from bin/copy/environment.json using Path and json.load() into a variable called \n",
    "# jenv and print the keys of the dictionary jenv\n",
    "\n",
    "# open file for read\n",
    "with json_file.open('r') as f:\n",
    "    jenv = json.load(f)\n",
    "    \n",
    "print(f'jenv keys: {jenv.keys()}')\n",
    "\n",
    "# use assert to check if the keys are the same\n",
    "assert jenv.keys() == env.keys()\n",
    "print('passed assertion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# ANSWER \n",
    "\n",
    "# print out the absolute pathname of the \n",
    "# directory that images/ucl.png is in\n",
    "abs_name = Path('images/ucl.png').absolute()\n",
    "print(abs_name)\n",
    "\n",
    "# we want the parent!\n",
    "print(f'the file {abs_name.name} is in {abs_name.parent}')\n",
    "\n",
    "# print the size of the file in bytes\n",
    "print(f'{abs_name.name} has size {abs_name.stat().st_size} bytes')\n",
    "\n",
    "# 1 KB is 1024 Bytes\n",
    "# .2f is 2 d.p. format\n",
    "print(f'{abs_name.name} has size ' +\\\n",
    "      f'{abs_name.stat().st_size/1024:.2f} KB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use this idea to make a dictionary of our ENSO dataset, using the items in the header for the keys. In this way, we obtain a  more elegant representation of the dataset, and can refer to items by names (keys) instead of column numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import io\n",
    "\n",
    "# copy the useful data\n",
    "start_head = txt.find('YEAR')\n",
    "start_data = txt.find('1950\\t')\n",
    "stop_data  = txt.find('2018\\t')\n",
    "\n",
    "header = txt[start_head:start_data].split()\n",
    "data = np.loadtxt(io.StringIO(txt[start_data:stop_data]),unpack=True)\n",
    "\n",
    "# use zip to load into a dictionary\n",
    "data_dict = dict(zip(header, data))\n",
    "\n",
    "key = 'MAYJUN'\n",
    "# plot data\n",
    "plt.figure(0,figsize=(12,7))\n",
    "plt.title('ENSO data from {0}'.format(url))\n",
    "plt.plot(data_dict['YEAR'],data_dict[key],label=key)\n",
    "plt.xlabel('year')\n",
    "plt.ylabel('ENSO')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "#### Exercise 1\n",
    "\n",
    "* copy the code above, and modify so that datasets for months `['MAYJUN','JUNJUL','JULAUG']` are plotted on the graph\n",
    "\n",
    "Hint: use a for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# do exercise here\n",
    "# ANSWER\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "import io\n",
    "\n",
    "# access dataset as above\n",
    "url = \"http://www.esrl.noaa.gov/psd/enso/mei.old/table.html\"\n",
    "txt = requests.get(url).text\n",
    "\n",
    "# copy the useful data\n",
    "start_head = txt.find('YEAR')\n",
    "start_data = txt.find('1950\\t')\n",
    "stop_data  = txt.find('2018\\t')\n",
    "\n",
    "header = txt[start_head:start_data].split()\n",
    "data = np.loadtxt(io.StringIO(txt[start_data:stop_data]),unpack=True)\n",
    "\n",
    "# use zip to load into a dictionary\n",
    "data_dict = dict(zip(header, data))\n",
    "\n",
    "\n",
    "'''\n",
    "Do the loop here\n",
    "'''\n",
    "for i,key in enumerate(['MAYJUN','JUNJUL','JULAUG']):\n",
    "    # plot data\n",
    "    '''\n",
    "    Use enumeration i as figure number\n",
    "    '''\n",
    "    plt.figure(i,figsize=(12,7))\n",
    "    plt.title('ENSO data from {0}'.format(url))\n",
    "    plt.plot(data_dict['YEAR'],data_dict[key],label=key)\n",
    "    plt.xlabel('year')\n",
    "    plt.ylabel('ENSO')\n",
    "    plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also usefully use a dictionary with a printing format statement. In that case, we refer directly to the key in ther format string. This can make printing statements much easier to read. We don;'t directly pass the dictionary to the `fortmat` staterment, but rather `**dict`, where `**dict` means \"treat the key-value pairs in the dictionary as additional named arguments to this function call\".\n",
    "\n",
    "So, in the example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import io\n",
    "\n",
    "# access dataset as above\n",
    "url = \"http://www.esrl.noaa.gov/psd/enso/mei/table.html\"\n",
    "txt = requests.get(url).text\n",
    "\n",
    "# copy the useful data\n",
    "start_head = txt.find('YEAR')\n",
    "start_data = txt.find('1950\\t')\n",
    "stop_data  = txt.find('2018\\t')\n",
    "\n",
    "header = txt[start_head:start_data].split()\n",
    "data = np.loadtxt(io.StringIO(txt[start_data:stop_data]),unpack=True)\n",
    "\n",
    "# use zip to load into a dictionary\n",
    "data_dict = dict(zip(header, data))\n",
    "print(data_dict.keys())\n",
    "\n",
    "# print the data for MAYJUN\n",
    "print('data for MAYJUN: {MAYJUN}'.format(**data_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line `print('data for MAYJUN: {MAYJUN}'.format(**data_dict))` is equivalent to writing:\n",
    "\n",
    "    print('data for {MAYJUN}'.format(YEAR=data_dict[YEAR],DECJAN=data_dict[DECJAN], ...))\n",
    "    \n",
    "In this way, we use the keys in the dictionary as keywords to pass to a method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful example of such a use of a dictionary is in saving a numpy dataset to file.\n",
    "\n",
    "If the data are numpy arrays in a dictionary as above, we can store the dataset using:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import io\n",
    "\n",
    "# access dataset as above\n",
    "url = \"http://www.esrl.noaa.gov/psd/enso/mei/table.html\"\n",
    "txt = requests.get(url).text\n",
    "\n",
    "# copy the useful data\n",
    "start_head = txt.find('YEAR')\n",
    "start_data = txt.find('1950\\t')\n",
    "stop_data  = txt.find('2018\\t')\n",
    "\n",
    "header = txt[start_head:start_data].split()\n",
    "data = np.loadtxt(io.StringIO(txt[start_data:stop_data]),unpack=True)\n",
    "\n",
    "# use zip to load into a dictionary\n",
    "data_dict = dict(zip(header, data))\n",
    "\n",
    "filename = 'enso_mei.npz'\n",
    "\n",
    "# save the dataset\n",
    "np.savez_compressed(filename,**data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we load from the file is a dictionary-like object `<class 'numpy.lib.npyio.NpzFile'>`.\n",
    "\n",
    "If needed, we can cast this to a dictionary with `dict()`, but it is generally more efficient to keep the original type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "\n",
    "filename = 'enso_mei.npz'\n",
    "\n",
    "loaded_data = np.load(filename)\n",
    "\n",
    "print(type(loaded_data))\n",
    "\n",
    "# test they are the same using np.array_equal\n",
    "for k in loaded_data.keys():\n",
    "    print('\\t',k,np.array_equal(data_dict[k], loaded_data[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown",
    "solution2_first": true
   },
   "source": [
    "#### Exercise 2\n",
    "\n",
    "* Using what you have learned above, access the Met Office data file [`https://www.metoffice.gov.uk/hadobs/hadukp/data/monthly/HadSEEP_monthly_qc.txt`](https://www.metoffice.gov.uk/hadobs/hadukp/data/monthly/HadSEEP_monthly_qc.txt) and create a 'data package' in a numpy`.npz` file that has keys of `YEAR` and each month in the year, with associated datasets of Monthly Southeast England precipitation (mm).\n",
    "* confirm that tha data in your `npz` file is the same as in your original dictionary\n",
    "* produce a plot of October rainfall using these data for the years 1900 onwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "shown"
   },
   "outputs": [],
   "source": [
    "# do exercise here\n",
    "# ANSWER\n",
    "\n",
    "'''\n",
    "Exploration of dataset shows:\n",
    "\n",
    "\n",
    "Monthly Southeast England precipitation (mm). Daily automated values used after 1996.\n",
    "Wigley & Jones (J.Climatol.,1987), Gregory et al. (Int.J.Clim.,1991)\n",
    "Jones & Conway (Int.J.Climatol.,1997), Alexander & Jones (ASL,2001). Values may change after QC.\n",
    "YEAR   JAN   FEB   MAR   APR   MAY   JUN   JUL   AUG   SEP   OCT   NOV   DEC   ANN\n",
    " 1873  87.1  50.4  52.9  19.9  41.1  63.6  53.2  56.4  62.0  86.0  59.4  15.7  647.7\n",
    " 1874  46.8  44.9  15.8  48.4  24.1  49.9  28.3  43.6  79.4  96.1  63.9  52.3  593.5\n",
    "\n",
    "so we have 3 lines of header\n",
    "then the column titles\n",
    "then the data\n",
    "\n",
    "we can define these as before using\n",
    "\n",
    "txt.find('YEAR')\n",
    "start_data = txt.find('1873')\n",
    "stop_data = None\n",
    "\n",
    "\n",
    "Other than the filenames then, the code\n",
    "is identical\n",
    "'''\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "import io\n",
    "\n",
    "# access dataset as above\n",
    "url = \"https://www.metoffice.gov.uk/hadobs/hadukp/data/monthly/HadSEEP_monthly_qc.txt\"\n",
    "txt = requests.get(url).text\n",
    "\n",
    "# copy the useful data\n",
    "start_head = txt.find('YEAR')\n",
    "start_data = txt.find('1873')\n",
    "stop_data  = None\n",
    "\n",
    "header = txt[start_head:start_data].split()\n",
    "data = np.loadtxt(io.StringIO(txt[start_data:stop_data]),unpack=True)\n",
    "\n",
    "# use zip to load into a dictionary\n",
    "data_dict = dict(zip(header, data))\n",
    "\n",
    "filename = 'HadSEEP_monthly_qc.npz'\n",
    "\n",
    "# save the dataset\n",
    "np.savez_compressed(filename,**data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "shown"
   },
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "\n",
    "loaded_data = np.load(filename)\n",
    "\n",
    "print(type(loaded_data))\n",
    "\n",
    "# test they are the same using np.array_equal\n",
    "for k in loaded_data.keys():\n",
    "    print('\\t',k,np.array_equal(data_dict[k], loaded_data[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "shown"
   },
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "\n",
    "'''\n",
    "October rainfall, 1900+\n",
    "'''\n",
    "\n",
    "year = loaded_data['YEAR']\n",
    "\n",
    "# mask where years match\n",
    "mask = year  >= 1900\n",
    "\n",
    "oct = loaded_data['OCT']\n",
    "\n",
    "# set invalid data points to nan\n",
    "oct[oct<0] = np.nan\n",
    "\n",
    "plt.plot(year[mask],oct[mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.5 Summary\n",
    "\n",
    "In this section, we have extended the types of data we might come across to include groups . We dealt with ordered groups of various types (`tuple`, `list`), and introduced the numpy package for numpy arrays (`np.array`). We saw dictionaries as collections with which we refer to individual items with a key.\n",
    "\n",
    "We learned in the previous section how to pull apart a dataset presented as a string using loops and various using methods and to construct a useful dataset 'by hand' in a list or similar structure. It is useful, when learning to program, to know how to do this.\n",
    "\n",
    "Here, we saw that packages such as numpy provide higher level routines that make reading data easier, and we would generally use these in practice. We saw how we can use `zip()` to help load a dataset from arrays into a dictionary, and also the value of using a dictionary representation when saving numpy files."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "207px"
   },
   "toc_section_display": false,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
